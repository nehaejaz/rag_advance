{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 0: Setup\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/wandb/edu/blob/main/rag-advanced/notebooks/Chapter00.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "<!--- @wandbcode{rag-course-00} -->\n",
    "\n",
    "Let's install the required packages and check our setup for this course.\n",
    "\n",
    "### üéâ Free Cohere API key\n",
    "\n",
    "Before you run this colab notebook, head over to this [link to redeem a free Cohere API key](https://docs.google.com/forms/d/e/1FAIpQLSc9x4nV8_nSQvJnaINO1j9NIa2IUbAJqrKeSllNNCCbMFmCxw/viewform?usp=sf_link).\n",
    "\n",
    "Alternatively if you have a Cohere API key feel free to proceed. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq weave cohere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Weave\n",
    "\n",
    "\n",
    "The code cell below will prompt you to put in a W&B API key. You can get your API key by heading over to https://wandb.ai/authorize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnehaejaz29\u001b[0m (\u001b[33mnehaejaz29-ontario-tech-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import weave\n",
    "import weave\n",
    "\n",
    "# initialize weave client\n",
    "weave_client = weave.init(\"rag_course\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup Cohere\n",
    "\n",
    "The code cell below will prompt you to put in a Cohere API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "import cohere\n",
    "\n",
    "cohere_client = cohere.ClientV2(\n",
    "    api_key=getpass.getpass(\"Please enter your COHERE_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple-turn chat with Cohere's command-r-plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üç© https://wandb.ai/nehaejaz29-ontario-tech-university/rag_course/r/call/0193bcdd-a252-7aa0-b5ae-f69ea58505d1\n",
      "id='74f80806-229e-413f-a694-2851f94b2b07' finish_reason='COMPLETE' prompt=None message=AssistantMessageResponse(role='assistant', tool_calls=None, tool_plan=None, content=[TextAssistantMessageResponseContentItem(type='text', text=\"Retrieval Augmented Generation (RAG) is a technique used in Natural Language Processing (NLP) that combines information retrieval with language generation. It aims to enhance the quality and factual accuracy of generated text by retrieving relevant information from an external source of knowledge, such as a large corpus of text documents or a structured knowledge base.\\n\\nHere's a simple breakdown of how RAG works:\\n1. **Information Retrieval**: RAG starts by taking a natural language input, often in the form of a question or a prompt. It then uses information retrieval techniques to search through a pre-defined source of knowledge and retrieve relevant pieces of information or documents.\\n2. **Generation**: Once the relevant information is retrieved, a language generation model, typically based on deep learning techniques like Transformer architectures (e.g., BERT, GPT), is used to generate a response. This generation process takes into account both the original input and the retrieved information.\\n3. **Integration**: The key aspect of RAG is how it integrates the retrieved information into the generation process. This can be done in various ways, such as concatenating the retrieved text with the input, using attention mechanisms to focus on relevant parts of the retrieved context, or conditioning the generation on the retrieved facts.\\n4. **Training**: RAG models are typically trained using a combination of language modeling objectives and more task-specific objectives, such as question answering or text generation. During training, the model learns to generate responses that are coherent, contextually appropriate, and factually consistent with the retrieved information.\\n\\nThe main advantage of RAG is that it helps language models provide more accurate and informative responses, especially when dealing with questions that require factual knowledge. By grounding the generation process in retrieved evidence, RAG reduces the tendency of language models to generate hallucinations (i.e., making things up) and improves their ability to incorporate relevant facts into the generated text.\\n\\nRAG has been applied in various NLP tasks, including question answering, dialogue generation, summarization, and text completion. It has also been used in building more robust and trustworthy language models, particularly in contexts where factual accuracy is of utmost importance, such as in healthcare or financial domains.\")], citations=None) usage=Usage(billed_units=UsageBilledUnits(input_tokens=9.0, output_tokens=446.0, search_units=None, classifications=None), tokens=UsageTokens(input_tokens=202.0, output_tokens=446.0)) logprobs=None\n"
     ]
    }
   ],
   "source": [
    "response = cohere_client.chat(\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What is retrieval augmented generation (RAG)?\"}\n",
    "    ],\n",
    "    model=\"command-r-plus\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=2000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's head over to the weave URL to check out the generated response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval Augmented Generation (RAG) is a technique used in Natural Language Processing (NLP) that combines information retrieval with language generation. It aims to enhance the quality and factual accuracy of generated text by retrieving relevant information from an external source of knowledge, such as a large corpus of text documents or a structured knowledge base.\n",
      "\n",
      "Here's a simple breakdown of how RAG works:\n",
      "1. **Information Retrieval**: RAG starts by taking a natural language input, often in the form of a question or a prompt. It then uses information retrieval techniques to search through a pre-defined source of knowledge and retrieve relevant pieces of information or documents.\n",
      "2. **Generation**: Once the relevant information is retrieved, a language generation model, typically based on deep learning techniques like Transformer architectures (e.g., BERT, GPT), is used to generate a response. This generation process takes into account both the original input and the retrieved information.\n",
      "3. **Integration**: The key aspect of RAG is how it integrates the retrieved information into the generation process. This can be done in various ways, such as concatenating the retrieved text with the input, using attention mechanisms to focus on relevant parts of the retrieved context, or conditioning the generation on the retrieved facts.\n",
      "4. **Training**: RAG models are typically trained using a combination of language modeling objectives and more task-specific objectives, such as question answering or text generation. During training, the model learns to generate responses that are coherent, contextually appropriate, and factually consistent with the retrieved information.\n",
      "\n",
      "The main advantage of RAG is that it helps language models provide more accurate and informative responses, especially when dealing with questions that require factual knowledge. By grounding the generation process in retrieved evidence, RAG reduces the tendency of language models to generate hallucinations (i.e., making things up) and improves their ability to incorporate relevant facts into the generated text.\n",
      "\n",
      "RAG has been applied in various NLP tasks, including question answering, dialogue generation, summarization, and text completion. It has also been used in building more robust and trustworthy language models, particularly in contexts where factual accuracy is of utmost importance, such as in healthcare or financial domains.\n"
     ]
    }
   ],
   "source": [
    "print(response.message.content[0].text)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "rag_ad_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
